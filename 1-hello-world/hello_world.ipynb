{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DepthAI - Hello World\n",
    "\n",
    "This notebook will read and display frames from the rgb camera and a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, import all necessary modules\n",
    "from pathlib import Path\n",
    "import blobconverter\n",
    "import cv2\n",
    "import depthai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these modules will enable us to build a simple event driven UI in the Jupyter notebook\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline tells DepthAI what operations to perform when running - you define all of the resources used and flows here\n",
    "pipeline = depthai.Pipeline()\n",
    "assert pipeline is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want the Color camera as the output\n",
    "cam_rgb = pipeline.createColorCamera()\n",
    "cam_rgb.setPreviewSize(300, 300)  # 300x300 will be the preview frame size, available as 'preview' output of the node\n",
    "cam_rgb.setInterleaved(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want a neural network that will produce the detections\n",
    "detection_nn = pipeline.createMobileNetDetectionNetwork()\n",
    "# Blob is the Neural Network file, compiled for MyriadX. It contains both the definition and weights of the model\n",
    "# We're using a blobconverter tool to retreive the MobileNetSSD blob automatically from OpenVINO Model Zoo\n",
    "detection_nn.setBlobPath(blobconverter.from_zoo(name='mobilenet-ssd', shaves=6))\n",
    "# Next, we filter out the detections that are below a confidence threshold. Confidence can be anywhere between <0..1>\n",
    "detection_nn.setConfidenceThreshold(0.5)\n",
    "# Next, we link the camera 'preview' output to the neural network detection input, so that it can produce detections\n",
    "cam_rgb.preview.link(detection_nn.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLinkOut is a \"way out\" from the device. Any data you want to transfer to host need to be send via XLink\n",
    "xout_rgb = pipeline.createXLinkOut()\n",
    "# For the rgb camera output, we want the XLink stream to be named \"rgb\"\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "# Linking camera preview to XLink input, so that the frames will be sent to host\n",
    "cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "# The same XLinkOut mechanism will be used to receive nn results\n",
    "xout_nn = pipeline.createXLinkOut()\n",
    "xout_nn.setStreamName(\"nn\")\n",
    "detection_nn.out.link(xout_nn.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showVideo(button):\n",
    "\n",
    "    # Pipeline is now finished, and we need to find an available device to run our pipeline\n",
    "    # we are using context manager here that will dispose the device after we stop using it\n",
    "    with depthai.Device(pipeline) as device:\n",
    "        # From this point, the Device will be in \"running\" mode and will start sending data via XLink\n",
    "\n",
    "        # To consume the device results, we get two output queues from the device, with stream names we assigned earlier\n",
    "        q_rgb = device.getOutputQueue(\"rgb\")\n",
    "        q_nn = device.getOutputQueue(\"nn\")\n",
    "\n",
    "        # Here, some of the default values are defined. Frame will be an image from \"rgb\" stream, detections will contain nn results\n",
    "        frame = None\n",
    "        detections = []\n",
    "\n",
    "        # Since the detections returned by nn have values from <0..1> range, they need to be multiplied by frame width/height to\n",
    "        # receive the actual position of the bounding box on the image\n",
    "        def frameNorm(frame, bbox):\n",
    "            normVals = np.full(len(bbox), frame.shape[0])\n",
    "            normVals[::2] = frame.shape[1]\n",
    "            return (np.clip(np.array(bbox), 0, 1) * normVals).astype(int)\n",
    "\n",
    "\n",
    "        # Main host-side application loop\n",
    "        while True:\n",
    "            # we try to fetch the data from nn/rgb queues. tryGet will return either the data packet or None if there isn't any\n",
    "            in_rgb = q_rgb.tryGet()\n",
    "            in_nn = q_nn.tryGet()\n",
    "\n",
    "            if in_rgb is not None:\n",
    "                # If the packet from RGB camera is present, we're retrieving the frame in OpenCV format using getCvFrame\n",
    "                frame = in_rgb.getCvFrame()\n",
    "\n",
    "            if in_nn is not None:\n",
    "                # when data from nn is received, we take the detections array that contains mobilenet-ssd results\n",
    "                detections = in_nn.detections\n",
    "\n",
    "            if frame is not None:\n",
    "                for detection in detections:\n",
    "                    # for each bounding box, we first normalize it to match the frame size\n",
    "                    bbox = frameNorm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))\n",
    "                    # and then draw a rectangle on the frame to show the actual result\n",
    "                    cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "\n",
    "                # convert the frame from a OpenCV numpy array to a jpeg \n",
    "                # so that we can display it in the Jupyter notebook.\n",
    "                _, jpg = cv2.imencode('.jpeg', frame)\n",
    "                \n",
    "                # show the frame on the screen.\n",
    "                display_handle.update(Image(data=jpg.tobytes()))\n",
    "                if stopButton.value==True:\n",
    "                    break                 \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the button widget\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger',\n",
    "    tooltip='Stop the Video',\n",
    "    icon='square'\n",
    ")\n",
    "\n",
    "# initialize the display window\n",
    "display_handle=display(None, display_id=True)\n",
    "# display the stop button\n",
    "display(stopButton)\n",
    "# create a thread that calls the showVideo() function above\n",
    "thread = threading.Thread(target=showVideo, args=(stopButton,))\n",
    "thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
